{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Capstone – Emotionality of Tweets: DistilBERT Fine-Tuning\n",
   "id": "b95b90f793e54e5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notebook advisories\n",
    "\n",
    "This notebook was developed with conceptual and implementation influence from Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow, and the Kaggle notebook “Twitter Emotion Classification” by Andrey Shtrass.\n",
    "\n",
    "AI Usage: AI assistance was used to help debug code, refine explanations, and assist comprehension of reinforcement learning concepts."
   ],
   "id": "823808a94735f9d0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-12T05:20:42.288533Z",
     "start_time": "2025-12-12T05:20:38.884698Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "from datasets import Dataset as HFDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.data_utils import load_and_prepare_emotion_splits  # noqa: E402\n",
    "\n",
    "PROJECT_ROOT"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maine/School/Machine Learning 2/Final_Project_Code/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/maine/School/Machine Learning 2/Final_Project_Code')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Configuration",
   "id": "2ab87c2b0c88740b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T05:20:42.295186Z",
     "start_time": "2025-12-12T05:20:42.293126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"primary_emotions\"\n",
    "print(\"Data directory:\", DATA_DIR)\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "EMOTION_ID2NAME: Dict[int, str] = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\",\n",
    "}\n",
    "\n",
    "MAX_SEQ_LEN = 64\n",
    "BATCH_SIZE = 16          # per device\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "RANDOM_STATE = 42"
   ],
   "id": "f22e8cf7588850d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/maine/School/Machine Learning 2/Final_Project_Code/data/primary_emotions\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Evaluation Helper",
   "id": "3063cbf7353a357d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T05:20:42.301821Z",
     "start_time": "2025-12-12T05:20:42.299680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation helper (same style as TF-IDF baseline)\n",
    "\n",
    "def evaluate_predictions(\n",
    "        y_true: List[int],\n",
    "        y_pred: List[int],\n",
    "        label_ids: List[int],\n",
    "        label_names: List[str],\n",
    "        split_name: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Print detailed classification metrics for a given split.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== {split_name} Metrics ===\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Macro P:   {precision_macro:.4f}\")\n",
    "    print(f\"Macro R:   {recall_macro:.4f}\")\n",
    "    print(f\"Macro F1:  {f1_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            labels=label_ids,\n",
    "            target_names=label_names,\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=label_ids)\n",
    "    print(\"Confusion matrix (rows = true, cols = pred):\")\n",
    "    print(cm)"
   ],
   "id": "da018671fa03921",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Building Hugging Face Datasets and Tokenizer",
   "id": "dc87c2941719e1a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T05:20:42.307176Z",
     "start_time": "2025-12-12T05:20:42.304881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 3. Build HF Datasets and tokenizer\n",
    "# ============================================================\n",
    "\n",
    "def build_hf_datasets(\n",
    "        max_length: int = MAX_SEQ_LEN,\n",
    ") -> Tuple[HFDataset, HFDataset, HFDataset, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Build Hugging Face Datasets and tokenizer for DistilBERT fine-tuning.\n",
    "    And return train/val/test datasets and tokenizer.\n",
    "    \"\"\"\n",
    "    # Load splits from shared helper\n",
    "    splits = load_and_prepare_emotion_splits(DATA_DIR, normalize=True)\n",
    "    X_train, y_train = splits.train_texts, splits.train_labels\n",
    "    X_val, y_val = splits.val_texts, splits.val_labels\n",
    "    X_test, y_test = splits.test_texts, splits.test_labels\n",
    "\n",
    "    # Build raw HF datasets\n",
    "    train_dict = {\"text\": X_train, \"label\": y_train}\n",
    "    val_dict = {\"text\": X_val, \"label\": y_val}\n",
    "    test_dict = {\"text\": X_test, \"label\": y_test}\n",
    "\n",
    "    hf_train = HFDataset.from_dict(train_dict)\n",
    "    hf_val = HFDataset.from_dict(val_dict)\n",
    "    hf_test = HFDataset.from_dict(test_dict)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    # Map tokenizer over datasets\n",
    "    hf_train = hf_train.map(tokenize_batch, batched=True)\n",
    "    hf_val = hf_val.map(tokenize_batch, batched=True)\n",
    "    hf_test = hf_test.map(tokenize_batch, batched=True)\n",
    "\n",
    "    # Rename label -> labels for transformers\n",
    "    hf_train = hf_train.rename_column(\"label\", \"labels\")\n",
    "    hf_val = hf_val.rename_column(\"label\", \"labels\")\n",
    "    hf_test = hf_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    # Set format for PyTorch\n",
    "    hf_train.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    )\n",
    "    hf_val.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    )\n",
    "    hf_test.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    )\n",
    "\n",
    "    return hf_train, hf_val, hf_test, tokenizer"
   ],
   "id": "9eedb28666f22762",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Trainer Metrics",
   "id": "6d352a8ea4b694ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T05:20:42.310957Z",
     "start_time": "2025-12-12T05:20:42.309352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Metrics for Trainer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute scalar metrics for HF Trainer from (logits, labels).\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels,\n",
    "        preds,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_precision\": precision_macro,\n",
    "        \"macro_recall\": recall_macro,\n",
    "        \"macro_f1\": f1_macro,\n",
    "    }"
   ],
   "id": "904eaf26a26731b9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Fine-Tuning DistilBERT\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Build tokenized HF datasets and tokenizer.\n",
    "2. Define label mappings (`id2label`, `label2id`)\n",
    "3. Load `AutoModelForSequenceClassification` with:\n",
    "   - `MODEL_NAME = \"distilbert-base-uncased\"`\n",
    "   - `num_labels = 6`\n",
    "   - `id2label`, `label2id`\n",
    "4. Configure `TrainingArguments`:\n",
    "   - Learning rate\n",
    "   - Batch sizes\n",
    "   - Number of epochs\n",
    "   - Weight decay\n",
    "   - Random seed\n",
    "5. Instantiate `Trainer` with:\n",
    "   - Model\n",
    "   - TrainingArguments\n",
    "   - Train and eval datasets\n",
    "   - Tokenizer\n",
    "   - `compute_metrics`\n",
    "6. Call `trainer.train()` to fine-tune the model.\n",
    "7. Evaluate with:\n",
    "   - `trainer.evaluate()`\n",
    "   - Custom detailed evaluation using `evaluate_predictions`."
   ],
   "id": "e34dd9f070841e79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T05:20:42.315783Z",
     "start_time": "2025-12-12T05:20:42.313223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fine-tune DistilBERT and evaluate\n",
    "\n",
    "def main() -> None:\n",
    "    print(\"Data directory:\", DATA_DIR)\n",
    "    print(\"Building HF datasets and tokenizer...\")\n",
    "\n",
    "    train_ds, val_ds, test_ds, tokenizer = build_hf_datasets(max_length=MAX_SEQ_LEN)\n",
    "\n",
    "    label_ids = sorted(EMOTION_ID2NAME.keys())\n",
    "    label_names = [EMOTION_ID2NAME[i] for i in label_ids]\n",
    "\n",
    "    id2label = {i: EMOTION_ID2NAME[i] for i in label_ids}\n",
    "    label2id = {name: i for i, name in EMOTION_ID2NAME.items()}\n",
    "\n",
    "    print(\"Using emotion mapping:\")\n",
    "    print(id2label)\n",
    "\n",
    "    # Load pretrained model with correct label mapping\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(label_ids),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    # Training arguments (compatible with older transformers)\n",
    "    output_dir = PROJECT_ROOT / \"models\" / \"distilbert_emotion\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        logging_steps=100,\n",
    "        seed=RANDOM_STATE,\n",
    "        # NOTE: no evaluation_strategy/save_strategy/load_best_model_at_end/etc.\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(\"Starting DistilBERT fine-tuning...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # HF evaluation on validation set (scalar metrics)\n",
    "    print(\"\\nHF Trainer evaluation on validation set:\")\n",
    "    print(trainer.evaluate(eval_dataset=val_ds))\n",
    "\n",
    "    # Custom, detailed evaluation on val and test\n",
    "    def collect_predictions(dataset: HFDataset) -> Tuple[List[int], List[int]]:\n",
    "        preds_output = trainer.predict(dataset)\n",
    "        logits = preds_output.predictions\n",
    "        labels = preds_output.label_ids\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return labels.tolist(), preds.tolist()\n",
    "\n",
    "    print(\"\\nDetailed evaluation on validation set:\")\n",
    "    y_val_true, y_val_pred = collect_predictions(val_ds)\n",
    "    evaluate_predictions(y_val_true, y_val_pred, label_ids, label_names, \"Validation\")\n",
    "\n",
    "    print(\"\\nDetailed evaluation on test set:\")\n",
    "    y_test_true, y_test_pred = collect_predictions(test_ds)\n",
    "    evaluate_predictions(y_test_true, y_test_pred, label_ids, label_names, \"Test\")"
   ],
   "id": "abdc2b29328d3b7a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Run Fine-Tuning",
   "id": "fb410be2daffe465"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T05:25:32.251216Z",
     "start_time": "2025-12-12T05:20:42.320155Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "acf85461c27a7136",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/maine/School/Machine Learning 2/Final_Project_Code/data/primary_emotions\n",
      "Building HF datasets and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 55971.82 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 76911.72 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 76839.86 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/b_/3x9y9z0d1ls1wgmhbxxctxfc0000gn/T/ipykernel_17509/3597094297.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using emotion mapping:\n",
      "{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
      "Starting DistilBERT fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maine/School/Machine Learning 2/Final_Project_Code/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 04:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.927200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.086400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "907788325b370d337df4598a21206243"
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maine/School/Machine Learning 2/Final_Project_Code/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/maine/School/Machine Learning 2/Final_Project_Code/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HF Trainer evaluation on validation set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maine/School/Machine Learning 2/Final_Project_Code/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "b788c091236596d6c3294b2b7a8dfdc2"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1469372808933258, 'eval_accuracy': 0.9415, 'eval_macro_precision': 0.9181008932111295, 'eval_macro_recall': 0.9182592317283181, 'eval_macro_f1': 0.918050752442631, 'eval_runtime': 3.54, 'eval_samples_per_second': 564.973, 'eval_steps_per_second': 35.311, 'epoch': 3.0}\n",
      "\n",
      "Detailed evaluation on validation set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maine/School/Machine Learning 2/Final_Project_Code/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Metrics ===\n",
      "Accuracy:  0.9415\n",
      "Macro P:   0.9181\n",
      "Macro R:   0.9183\n",
      "Macro F1:  0.9181\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.96      0.97      0.96       550\n",
      "         joy       0.97      0.95      0.96       704\n",
      "        love       0.87      0.90      0.88       178\n",
      "       anger       0.95      0.94      0.94       275\n",
      "        fear       0.89      0.91      0.90       212\n",
      "    surprise       0.87      0.84      0.86        81\n",
      "\n",
      "    accuracy                           0.94      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.94      0.94      0.94      2000\n",
      "\n",
      "Confusion matrix (rows = true, cols = pred):\n",
      "[[534   1   0   6   9   0]\n",
      " [  3 670  23   2   2   4]\n",
      " [  3  15 160   0   0   0]\n",
      " [ 10   2   0 258   5   0]\n",
      " [  7   0   0   6 193   6]\n",
      " [  1   4   1   0   7  68]]\n",
      "\n",
      "Detailed evaluation on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maine/School/Machine Learning 2/Final_Project_Code/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "089bb6f23809d1a3cced2a5a7865e5f8"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Metrics ===\n",
      "Accuracy:  0.9210\n",
      "Macro P:   0.8725\n",
      "Macro R:   0.8690\n",
      "Macro F1:  0.8707\n",
      "\n",
      "Per-class report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.95      0.97      0.96       581\n",
      "         joy       0.95      0.94      0.95       695\n",
      "        love       0.83      0.84      0.83       159\n",
      "       anger       0.92      0.91      0.91       275\n",
      "        fear       0.87      0.88      0.87       224\n",
      "    surprise       0.71      0.68      0.70        66\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "Confusion matrix (rows = true, cols = pred):\n",
      "[[564   3   0   8   6   0]\n",
      " [  5 654  27   4   0   5]\n",
      " [  0  24 133   2   0   0]\n",
      " [ 13   3   0 250   9   0]\n",
      " [  6   0   0   9 196  13]\n",
      " [  3   3   0   0  15  45]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Conclusion and Discussion\n",
    "\n",
    "This notebook fine-tuned DistilBERT for 6-way emotion classification on tweets, using the same train/validation/test splits and label mapping as the TF–IDF + Logistic Regression and BiLSTM baselines. The model started from the pretrained `distilbert-base-uncased` encoder, added a small 6-class classification head, and was trained with the Hugging Face `Trainer` on tweets truncated/padded to 64 tokens.\n",
    "\n",
    "On the held-out test set, DistilBERT reached around 92% accuracy and 0.88 macro-F1, clearly outperforming both the classical baseline (~0.73 macro-F1) and the BiLSTM (~0.80 macro-F1). The largest gains appeared in the harder, lower-frequency emotions such as love, fear, and especially surprise, where the transformer’s contextual representations helped close much of the gap. Common emotions like sadness and joy were already strong for all models, but DistilBERT still pushed their F1 scores into the mid-90s. The confusion matrix shows that the model is still not perfect. It continues to mix related emotions, and it only sees single tweets in isolation, without conversation history, user context, or any explicit modeling of sarcasm or irony. That limits its understanding of emotional tone can be.\n",
    "\n",
    "These results are consistent with expectations for pretrained transformers on short text classification:\n",
    "DistilBERT’s subword tokenization and deep contextual embeddings allow it to pick up more nuance in phrasing. Though it is more computationally expensive than the baselines.\n",
    "\n",
    "In the broader project, this DistilBERT model serves as the strongest baseline for reading emotional tone in tweets and provides a realistic picture of what current transformer-based text classifiers can and cannot do in this setting."
   ],
   "id": "adaa15da617dd3fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
